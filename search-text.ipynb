{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 경로 설정\n",
    "folder_path = 'text_to_json'\n",
    "\n",
    "# manual_text 리스트 초기화\n",
    "manual_text = []\n",
    "\n",
    "# JSON 파일들을 순차적으로 읽기\n",
    "for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        manual_text.append(data)\n",
    "\n",
    "# 결과 확인\n",
    "for text in manual_text:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example manual text and query\n",
    "\n",
    "query = \"Explain about the image including Safety net\"\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Calculate IDF for each term\n",
    "def compute_idf(corpus):\n",
    "    df = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        terms = set(tokenize(document))\n",
    "        for term in terms:\n",
    "            df[term] += 1\n",
    "    total_docs = len(corpus)\n",
    "    idf = {term: math.log((total_docs + 1) / (df_value + 1)) + 1 for term, df_value in df.items()}\n",
    "    return idf\n",
    "\n",
    "# Calculate BM25 score for a document given a query\n",
    "def compute_bm25(doc, query, idf, avgdl, k1=1.5, b=0.75):\n",
    "    doc_terms = tokenize(doc)\n",
    "    query_terms = tokenize(query)\n",
    "    doc_len = len(doc_terms)\n",
    "    term_freq = Counter(doc_terms)\n",
    "    \n",
    "    score = 0.0\n",
    "    for term in query_terms:\n",
    "        if term in term_freq:\n",
    "            f = term_freq[term]\n",
    "            idf_term = idf.get(term, 0)\n",
    "            score += idf_term * ((f * (k1 + 1)) / (f + k1 * (1 - b + b * (doc_len / avgdl))))\n",
    "    return score\n",
    "\n",
    "# Preprocess the manual\n",
    "corpus = [tokenize(doc) for doc in manual_text]\n",
    "avgdl = sum(len(doc) for doc in corpus) / len(corpus)\n",
    "idf = compute_idf(manual_text)\n",
    "\n",
    "# Compute scores for each document\n",
    "scores = [(doc, compute_bm25(doc, query, idf, avgdl)) for doc in manual_text]\n",
    "ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output the top ranked documents\n",
    "for doc, score in ranked_docs:\n",
    "    print(f\"Score: {score}\\nDocument: {doc}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPR 모델과 토크나이저 로드\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# 문서들을 벡터로 인코딩\n",
    "context_vectors = []\n",
    "for text in manual_text:\n",
    "    inputs = context_tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        context_vector = context_encoder(**inputs).pooler_output\n",
    "    context_vectors.append(context_vector.cpu().numpy())\n",
    "\n",
    "context_vectors = np.vstack(context_vectors)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "index = faiss.IndexFlatIP(context_vectors.shape[1])\n",
    "index.add(context_vectors)\n",
    "\n",
    "# 검색 쿼리\n",
    "query = \"Explain about the image including Safety net\"\n",
    "\n",
    "# 쿼리를 벡터로 인코딩\n",
    "query_inputs = question_tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "with torch.no_grad():\n",
    "    query_vector = question_encoder(**query_inputs).pooler_output.cpu().numpy()\n",
    "\n",
    "# FAISS 인덱스를 사용하여 유사한 문서 검색\n",
    "k = 5  # 상위 5개의 결과를 가져옵니다.\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"Top 5 results:\")\n",
    "for idx in indices[0]:\n",
    "    print(f\"Score: {distances[0][idx]}\\nDocument: {manual_text[idx]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
